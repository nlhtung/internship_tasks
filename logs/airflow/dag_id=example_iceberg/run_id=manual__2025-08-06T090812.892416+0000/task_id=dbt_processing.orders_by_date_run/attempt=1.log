[2025-08-06T09:08:41.177+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: example_iceberg.dbt_processing.orders_by_date_run manual__2025-08-06T09:08:12.892416+00:00 [queued]>
[2025-08-06T09:08:41.185+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: example_iceberg.dbt_processing.orders_by_date_run manual__2025-08-06T09:08:12.892416+00:00 [queued]>
[2025-08-06T09:08:41.186+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-08-06T09:08:41.196+0000] {taskinstance.py:1382} INFO - Executing <Task(DbtRunLocalOperator): dbt_processing.orders_by_date_run> on 2025-08-06 09:08:12.892416+00:00
[2025-08-06T09:08:41.203+0000] {standard_task_runner.py:57} INFO - Started process 215 to run task
[2025-08-06T09:08:41.206+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'example_iceberg', 'dbt_processing.orders_by_date_run', 'manual__2025-08-06T09:08:12.892416+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/dags_***/ex.py', '--cfg-path', '/tmp/tmpssfdgo5m']
[2025-08-06T09:08:41.207+0000] {standard_task_runner.py:85} INFO - Job 7: Subtask dbt_processing.orders_by_date_run
[2025-08-06T09:08:41.268+0000] {task_command.py:416} INFO - Running <TaskInstance: example_iceberg.dbt_processing.orders_by_date_run manual__2025-08-06T09:08:12.892416+00:00 [running]> on host cd1ed1f972af
[2025-08-06T09:08:41.357+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='example_iceberg' AIRFLOW_CTX_TASK_ID='dbt_processing.orders_by_date_run' AIRFLOW_CTX_EXECUTION_DATE='2025-08-06T09:08:12.892416+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-06T09:08:12.892416+00:00'
[2025-08-06T09:08:42.322+0000] {local.py:244} INFO - dbtRunner is available. Using dbtRunner for invoking dbt.
[2025-08-06T09:08:42.323+0000] {local.py:464} INFO - Cloning project to writable temp directory /tmp/tmpnrvueikj from /opt/airflow/dags/dbt_main_project
[2025-08-06T09:08:42.327+0000] {local.py:484} INFO - Partial parse is enabled and the latest partial parse file is /tmp/cosmos/example_iceberg__dbt_processing/target/partial_parse.msgpack
[2025-08-06T09:08:42.334+0000] {config.py:364} INFO - Profile caching is enable.
[2025-08-06T09:08:42.335+0000] {base.py:73} INFO - Using connection ID 'dbt_spark' for task execution.
[2025-08-06T09:08:42.336+0000] {config.py:344} INFO - Profile found in cache using profile: /tmp/cosmos/profile/818394f1fa07916eb06bcd5299ec520a7f75aff7aab6c09e4f1c608b1ae98e27/profiles.yml.
[2025-08-06T09:08:42.337+0000] {runner.py:60} INFO - Trying to run dbtRunner with:
 ['run', '--vars', "processing_date: '1970-01-01'\n", '--models', 'orders_by_date', '--project-dir', '/tmp/tmpnrvueikj', '--profiles-dir', '/tmp/cosmos/profile/818394f1fa07916eb06bcd5299ec520a7f75aff7aab6c09e4f1c608b1ae98e27', '--profile', 'dbt_main_project', '--target', 'dev', '--no-static-parser']
 in /tmp/tmpnrvueikj
[2025-08-06T09:08:42.401+0000] {logging_mixin.py:154} INFO - 09:08:42  Running with dbt=1.9.0-b2
[2025-08-06T09:08:42.653+0000] {logging_mixin.py:154} INFO - 09:08:42  Registered adapter: spark=1.8.0
[2025-08-06T09:08:42.995+0000] {logging_mixin.py:154} INFO - 09:08:42  Found 1 model, 1 seed, 463 macros
[2025-08-06T09:08:42.998+0000] {logging_mixin.py:154} INFO - 09:08:42
[2025-08-06T09:08:42.999+0000] {logging_mixin.py:154} INFO - 09:08:42  Concurrency: 1 threads (target='dev')
[2025-08-06T09:08:42.999+0000] {logging_mixin.py:154} INFO - 09:08:42
[2025-08-06T09:08:43.355+0000] {logging_mixin.py:154} INFO - 09:08:43  1 of 1 START sql table model default.orders_by_date ............................ [RUN]
[2025-08-06T09:08:45.639+0000] {logging_mixin.py:154} INFO - 09:08:45  1 of 1 OK created sql table model default.orders_by_date ....................... [OK in 2.28s]
[2025-08-06T09:08:45.679+0000] {logging_mixin.py:154} INFO - 09:08:45
[2025-08-06T09:08:45.680+0000] {logging_mixin.py:154} INFO - 09:08:45  Finished running 1 table model in 0 hours 0 minutes and 2.68 seconds (2.68s).
[2025-08-06T09:08:45.709+0000] {logging_mixin.py:154} INFO - 09:08:45
[2025-08-06T09:08:45.711+0000] {logging_mixin.py:154} INFO - 09:08:45  Completed successfully
[2025-08-06T09:08:45.712+0000] {logging_mixin.py:154} INFO - 09:08:45
[2025-08-06T09:08:45.712+0000] {logging_mixin.py:154} INFO - 09:08:45  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[2025-08-06T09:08:46.736+0000] {local.py:110} WARNING - Artifact schema version: https://schemas.getdbt.com/dbt/manifest/v12.json is above dbt-ol supported version 7. This might cause errors.
[2025-08-06T09:08:46.737+0000] {local.py:110} WARNING - Artifact schema version: https://schemas.getdbt.com/dbt/run-results/v6.json is above dbt-ol supported version 5. This might cause errors.
[2025-08-06T09:08:46.745+0000] {local.py:693} WARNING - 
                    Airflow 3.0.0 Asset (Dataset) URIs validation rules changed and OpenLineage URIs (standard used by Cosmos) will no longer be valid.
                    Therefore, if using Cosmos with Airflow 3, the Airflow Dataset URIs will be changed to <spark://spark-thrift-server:10000/None/default/orders_by_date>.
                    Previously, with Airflow 2.x, the URI was <spark://spark-thrift-server:10000/None.default.orders_by_date>.
                    If you want to use the Airflow 3 URI standard while still using Airflow 2, please, set:
                        export AIRFLOW__COSMOS__USE_DATASET_AIRFLOW3_URI_STANDARD=1
                    Remember to update any DAGs that are scheduled using this dataset.
                    
[2025-08-06T09:08:46.746+0000] {local.py:538} INFO - Inlets: []
[2025-08-06T09:08:46.747+0000] {local.py:539} INFO - Outlets: [Dataset(uri='spark://spark-thrift-server:10000/None.default.orders_by_date', extra=None)]
[2025-08-06T09:08:46.747+0000] {local.py:761} INFO - Assigning inlets/outlets without DatasetAlias
[2025-08-06T09:08:46.748+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2025-08-06T09:08:46.778+0000] {dag.py:3722} INFO - Setting next_dagrun for example_iceberg to 2025-08-06T00:00:00+00:00, run_after=2025-08-07T00:00:00+00:00
[2025-08-06T09:08:46.817+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.8/site-packages/***/models/baseoperator.py:1471: RemovedInAirflow3Warning: Passing 'execution_date' to 'TaskInstance.xcom_push()' is deprecated.
  context["ti"].xcom_push(key=key, value=value, execution_date=execution_date)

[2025-08-06T09:08:46.841+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=example_iceberg, task_id=dbt_processing.orders_by_date_run, execution_date=20250806T090812, start_date=20250806T090841, end_date=20250806T090846
[2025-08-06T09:08:46.875+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-08-06T09:08:46.897+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
