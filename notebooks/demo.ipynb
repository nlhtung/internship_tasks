{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd80d1a2-5da8-4f91-ade1-ebfae1e44fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/06 07:45:18 WARN DependencyUtils: Local jar /home/jovyan/jars/iceberg-spark-runtime-3.5_2.12-1.9.2.jar does not exist, skipping.\n",
      "25/08/06 07:45:18 INFO SparkContext: Running Spark version 3.4.1\n",
      "25/08/06 07:45:18 INFO ResourceUtils: ==============================================================\n",
      "25/08/06 07:45:18 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/08/06 07:45:18 INFO ResourceUtils: ==============================================================\n",
      "25/08/06 07:45:18 INFO SparkContext: Submitted application: IcebergHiveCatalogExample\n",
      "25/08/06 07:45:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/08/06 07:45:18 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/08/06 07:45:18 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/08/06 07:45:18 INFO SecurityManager: Changing view acls to: root,spark\n",
      "25/08/06 07:45:18 INFO SecurityManager: Changing modify acls to: root,spark\n",
      "25/08/06 07:45:18 INFO SecurityManager: Changing view acls groups to: \n",
      "25/08/06 07:45:18 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/08/06 07:45:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY\n",
      "25/08/06 07:45:18 INFO Utils: Successfully started service 'sparkDriver' on port 42807.\n",
      "25/08/06 07:45:18 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/08/06 07:45:18 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/08/06 07:45:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/08/06 07:45:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/08/06 07:45:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/08/06 07:45:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-717dd4f2-7da3-41d8-a680-425ad2f84d04\n",
      "25/08/06 07:45:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/08/06 07:45:19 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/08/06 07:45:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/08/06 07:45:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/08/06 07:45:19 ERROR SparkContext: Failed to add /home/jovyan/jars/iceberg-spark-runtime-3.5_2.12-1.9.2.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/jovyan/jars/iceberg-spark-runtime-3.5_2.12-1.9.2.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/08/06 07:45:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "25/08/06 07:45:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.7:7077 after 20 ms (0 ms spent in bootstraps)\n",
      "25/08/06 07:45:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250806074519-0001\n",
      "25/08/06 07:45:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250806074519-0001/0 on worker-20250806074102-172.18.0.8-43353 (172.18.0.8:43353) with 12 core(s)\n",
      "25/08/06 07:45:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20250806074519-0001/0 on hostPort 172.18.0.8:43353 with 12 core(s), 1024.0 MiB RAM\n",
      "25/08/06 07:45:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46863.\n",
      "25/08/06 07:45:19 INFO NettyBlockTransferService: Server created on 701b21a81e6e:46863\n",
      "25/08/06 07:45:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/08/06 07:45:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 701b21a81e6e, 46863, None)\n",
      "25/08/06 07:45:19 INFO BlockManagerMasterEndpoint: Registering block manager 701b21a81e6e:46863 with 434.4 MiB RAM, BlockManagerId(driver, 701b21a81e6e, 46863, None)\n",
      "25/08/06 07:45:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 701b21a81e6e, 46863, None)\n",
      "25/08/06 07:45:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 701b21a81e6e, 46863, None)\n",
      "25/08/06 07:45:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250806074519-0001/0 is now RUNNING\n",
      "25/08/06 07:45:19 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/08/06 07:45:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/08/06 07:45:20 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/work/spark-warehouse'.\n",
      "25/08/06 07:45:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:54710) with ID 0,  ResourceProfileId 0\n",
      "25/08/06 07:45:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:46295 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.8, 46295, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dữ liệu mẫu ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:23 INFO CodeGenerator: Code generated in 234.179755 ms\n",
      "25/08/06 07:45:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 701b21a81e6e:46863 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/08/06 07:45:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/08/06 07:45:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7420 bytes) \n",
      "25/08/06 07:45:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:46295 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1351 ms on 172.18.0.8 (executor 0) (1/1)\n",
      "25/08/06 07:45:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:25 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38213\n",
      "25/08/06 07:45:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.505 s\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.538523 s\n",
      "25/08/06 07:45:25 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 701b21a81e6e:46863 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "25/08/06 07:45:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/08/06 07:45:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7430 bytes) \n",
      "25/08/06 07:45:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:46295 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on 172.18.0.8 (executor 0) (1/1)\n",
      "25/08/06 07:45:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:25 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.121 s\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/08/06 07:45:25 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.126791 s\n",
      "25/08/06 07:45:25 INFO CodeGenerator: Code generated in 20.362932 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n",
      "|id |name                   |age|\n",
      "+---+-----------------------+---+\n",
      "|1  |Nguyễn Văn A           |30 |\n",
      "|2  |Trần Thị B             |25 |\n",
      "|3  |Lê Văn C               |28 |\n",
      "|4  |Nguyễn Lương Hoàng Tùng|21 |\n",
      "+---+-----------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:25 INFO HiveConf: Found configuration file null\n",
      "25/08/06 07:45:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083\n",
      "25/08/06 07:45:25 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "25/08/06 07:45:25 INFO metastore: Connected to metastore.\n",
      "25/08/06 07:45:26 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/08/06 07:45:26 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "25/08/06 07:45:26 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/08/06 07:45:26 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/08/06 07:45:26 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
      "25/08/06 07:45:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 701b21a81e6e:46863 in memory (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.8:46295 in memory (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 701b21a81e6e:46863 in memory (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.8:46295 in memory (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:29 INFO HiveTableOperations: Committed to table hive_catalog.default.users with the new metadata location s3a://iceberg-warehouse/users/metadata/00000-ab630dbe-8080-4063-b30a-c29d1fa69671.metadata.json\n",
      "25/08/06 07:45:29 INFO BaseMetastoreTableOperations: Successfully committed to table hive_catalog.default.users in 3448 ms\n",
      "25/08/06 07:45:29 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://iceberg-warehouse/users/metadata/00000-ab630dbe-8080-4063-b30a-c29d1fa69671.metadata.json\n",
      "25/08/06 07:45:29 INFO CodeGenerator: Code generated in 7.643613 ms\n",
      "25/08/06 07:45:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 701b21a81e6e:46863 (size: 29.4 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:29 INFO SparkContext: Created broadcast 2 from broadcast at SparkWrite.java:193\n",
      "25/08/06 07:45:29 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=hive_catalog.default.users, format=PARQUET). The input RDD has 2 partitions.\n",
      "25/08/06 07:45:29 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Got job 2 (append at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Final stage: ResultStage 2 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.9 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 701b21a81e6e:46863 (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/08/06 07:45:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "25/08/06 07:45:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7420 bytes) \n",
      "25/08/06 07:45:29 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7430 bytes) \n",
      "25/08/06 07:45:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:46295 (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.8:46295 (size: 29.4 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:32 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 3147 ms on 172.18.0.8 (executor 0) (1/2)\n",
      "25/08/06 07:45:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3149 ms on 172.18.0.8 (executor 0) (2/2)\n",
      "25/08/06 07:45:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:32 INFO DAGScheduler: ResultStage 2 (append at NativeMethodAccessorImpl.java:0) finished in 3.162 s\n",
      "25/08/06 07:45:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/08/06 07:45:32 INFO DAGScheduler: Job 2 finished: append at NativeMethodAccessorImpl.java:0, took 3.167965 s\n",
      "25/08/06 07:45:32 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=hive_catalog.default.users, format=PARQUET) is committing.\n",
      "25/08/06 07:45:32 INFO SparkWrite: Committing append with 2 new data files to table hive_catalog.default.users\n",
      "25/08/06 07:45:33 INFO HiveTableOperations: Committed to table hive_catalog.default.users with the new metadata location s3a://iceberg-warehouse/users/metadata/00001-973d658e-6d13-4a89-86da-5336a74ac354.metadata.json\n",
      "25/08/06 07:45:33 INFO BaseMetastoreTableOperations: Successfully committed to table hive_catalog.default.users in 245 ms\n",
      "25/08/06 07:45:33 INFO SnapshotProducer: Committed snapshot 1847557235031030640 (MergeAppend)\n",
      "25/08/06 07:45:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://iceberg-warehouse/users/metadata/00001-973d658e-6d13-4a89-86da-5336a74ac354.metadata.json\n",
      "25/08/06 07:45:33 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=hive_catalog.default.users, snapshotId=1847557235031030640, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.574126601S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=4}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=4}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1961}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1961}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.4.1, app-id=app-20250806074519-0001, engine-name=spark, iceberg-version=Apache Iceberg 1.9.2 (commit 071d5606bc6199a0be9b3f274ec7fbf111d88821)}}\n",
      "25/08/06 07:45:33 INFO SparkWrite: Committed in 642 ms\n",
      "25/08/06 07:45:33 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=hive_catalog.default.users, format=PARQUET) committed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Đã ghi dữ liệu vào hive_catalog.default.users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:33 INFO V2ScanRelationPushDown: \n",
      "Output: id#25, name#26, age#27\n",
      "         \n",
      "25/08/06 07:45:33 INFO SnapshotScan: Scanning table hive_catalog.default.users snapshot 1847557235031030640 created at 2025-08-06T07:45:33.258+00:00 with filter true\n",
      "25/08/06 07:45:33 INFO BaseDistributedDataScan: Planning file tasks locally for table hive_catalog.default.users\n",
      "25/08/06 07:45:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 701b21a81e6e:46863 in memory (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.8:46295 in memory (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 701b21a81e6e:46863 in memory (size: 29.4 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.8:46295 in memory (size: 29.4 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:33 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.default.users\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 701b21a81e6e:46863 (size: 29.6 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:34 INFO SparkContext: Created broadcast 4 from broadcast at SparkBatch.java:85\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 701b21a81e6e:46863 (size: 29.7 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO SparkContext: Created broadcast 5 from broadcast at SparkBatch.java:85\n",
      "25/08/06 07:45:34 INFO CodeGenerator: Code generated in 26.26152 ms\n",
      "25/08/06 07:45:34 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 15.0 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 701b21a81e6e:46863 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:34 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/08/06 07:45:34 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/08/06 07:45:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 12141 bytes) \n",
      "25/08/06 07:45:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.8:46295 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.8:46295 (size: 29.7 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 724 ms on 172.18.0.8 (executor 0) (1/1)\n",
      "25/08/06 07:45:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:34 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.750 s\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/08/06 07:45:34 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.754737 s\n",
      "25/08/06 07:45:34 INFO CodeGenerator: Code generated in 11.704356 ms            \n",
      "25/08/06 07:45:34 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://iceberg-warehouse/users/metadata/00001-973d658e-6d13-4a89-86da-5336a74ac354.metadata.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n",
      "|id |name                   |age|\n",
      "+---+-----------------------+---+\n",
      "|1  |Nguyễn Văn A           |30 |\n",
      "|2  |Trần Thị B             |25 |\n",
      "|3  |Lê Văn C               |28 |\n",
      "|4  |Nguyễn Lương Hoàng Tùng|21 |\n",
      "+---+-----------------------+---+\n",
      "\n",
      "=== Dữ liệu trong bảng Iceberg ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:34 INFO BaseMetastoreCatalog: Table loaded by catalog: hive_catalog.default.users.history\n",
      "25/08/06 07:45:35 INFO V2ScanRelationPushDown: \n",
      "Output: made_current_at#52, snapshot_id#53L, parent_id#54L, is_current_ancestor#55\n",
      "         \n",
      "25/08/06 07:45:35 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.default.users.history\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 701b21a81e6e:46863 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 701b21a81e6e:46863 (size: 29.7 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 7 from broadcast at SparkBatch.java:85\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.8:46295 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 701b21a81e6e:46863 (size: 29.7 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 8 from broadcast at SparkBatch.java:85\n",
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 12.833818 ms\n",
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 8.417291 ms\n",
      "25/08/06 07:45:35 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.8 KiB, free 434.1 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 701b21a81e6e:46863 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/08/06 07:45:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 11439 bytes) \n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.8:46295 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.8:46295 (size: 29.7 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 188 ms on 172.18.0.8 (executor 0) (1/1)\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:35 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.198 s\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.202165 s\n",
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 13.849788 ms\n",
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 11.54369 ms\n",
      "25/08/06 07:45:35 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://iceberg-warehouse/users/metadata/00001-973d658e-6d13-4a89-86da-5336a74ac354.metadata.json\n",
      "25/08/06 07:45:35 INFO BaseMetastoreCatalog: Table loaded by catalog: hive_catalog.default.users.snapshots\n",
      "25/08/06 07:45:35 INFO V2ScanRelationPushDown: \n",
      "Output: committed_at#86, snapshot_id#87L, parent_id#88L, operation#89, manifest_list#90, summary#91\n",
      "         \n",
      "25/08/06 07:45:35 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.default.users.snapshots\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 434.1 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 701b21a81e6e:46863 (size: 29.8 KiB, free: 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 701b21a81e6e:46863 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 10 from broadcast at SparkBatch.java:85\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.8:46295 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 434.0 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 701b21a81e6e:46863 (size: 29.8 KiB, free: 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 11 from broadcast at SparkBatch.java:85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2025-08-06 07:45:33.258|1847557235031030640|null     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n",
      "=== Lịch sử commit của bảng (mới nhất trước) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 24.149128 ms\n",
      "25/08/06 07:45:35 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.3 KiB, free 434.0 MiB)\n",
      "25/08/06 07:45:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.0 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 701b21a81e6e:46863 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "25/08/06 07:45:35 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/08/06 07:45:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 12562 bytes) \n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.8:46295 (size: 6.3 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.8:46295 (size: 29.8 KiB, free: 434.3 MiB)\n",
      "25/08/06 07:45:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 143 ms on 172.18.0.8 (executor 0) (1/1)\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/08/06 07:45:35 INFO DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.158 s\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/08/06 07:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/08/06 07:45:35 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.160619 s\n",
      "25/08/06 07:45:35 INFO CodeGenerator: Code generated in 15.858049 ms\n",
      "25/08/06 07:45:35 INFO SparkContext: SparkContext is stopping with exitCode 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|2025-08-06 07:45:...|1847557235031030640|     null|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "\n",
      "=== Danh sách snapshot hiện tại ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 07:45:35 INFO SparkUI: Stopped Spark web UI at http://701b21a81e6e:4040\n",
      "25/08/06 07:45:35 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "25/08/06 07:45:35 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "25/08/06 07:45:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/08/06 07:45:35 INFO MemoryStore: MemoryStore cleared\n",
      "25/08/06 07:45:35 INFO BlockManager: BlockManager stopped\n",
      "25/08/06 07:45:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/08/06 07:45:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/08/06 07:45:35 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "def main():\n",
    "   # Khởi SparkSession (nạp spark-defaults.conf và hive-site.xml)\n",
    "   spark = SparkSession.builder \\\n",
    "       .appName(\"IcebergHiveCatalogExample\") \\\n",
    "       .master(\"spark://spark-master:7077\") \\\n",
    "       .enableHiveSupport() \\\n",
    "       .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "       .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "       .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "       .config(\"spark.jars\", \"/home/jovyan/jars/iceberg-spark-runtime-3.5_2.12-1.9.2.jar\") \\\n",
    "       .getOrCreate()\n",
    "   \n",
    "   # Tạo DataFrame mẫu\n",
    "   schema = StructType([\n",
    "       StructField(\"id\",   IntegerType(), False),\n",
    "       StructField(\"name\", StringType(),  False),\n",
    "       StructField(\"age\",  IntegerType(), True)\n",
    "   ])\n",
    "   data = [\n",
    "       (1, \"Nguyễn Văn A\", 30),\n",
    "       (2, \"Trần Thị B\",   25),\n",
    "       (3, \"Lê Văn C\",     28),\n",
    "       (4, \"Nguyễn Lương Hoàng Tùng\", 21)\n",
    "   ]\n",
    "   df = spark.createDataFrame(data, schema)\n",
    "   \n",
    "   print(\"=== Dữ liệu mẫu ===\")\n",
    "   df.show(truncate=False)\n",
    "   \n",
    "   # Tạo namespace (database) nếu chưa có\n",
    "   spark.sql(\"CREATE NAMESPACE IF NOT EXISTS hive_catalog.default\")\n",
    "   \n",
    "   # Tạo table Iceberg (nếu chưa tồn tại)\n",
    "   spark.sql(\"\"\"\n",
    "     CREATE TABLE IF NOT EXISTS hive_catalog.default.users (\n",
    "       id   INT,\n",
    "       name STRING,\n",
    "       age  INT\n",
    "     ) USING iceberg\n",
    "   \"\"\")\n",
    "   \n",
    "   # Ghi dữ liệu vào bảng (append)\n",
    "   df.writeTo(\"hive_catalog.default.users\").append()\n",
    "   print(\">>> Đã ghi dữ liệu vào hive_catalog.default.users\")\n",
    "   \n",
    "   # Đọc lại và hiển thị\n",
    "   spark.table(\"hive_catalog.default.users\").show(truncate=False)\n",
    "   print(\"=== Dữ liệu trong bảng Iceberg ===\")\n",
    "   \n",
    "   # Hiển thị lịch sử commit, sắp xếp theo made_current_at DESC\n",
    "   hist_df = spark.table(\"hive_catalog.default.users.history\") \\\n",
    "                 .orderBy(col(\"made_current_at\").desc())\n",
    "   hist_df.show(10, truncate=False)\n",
    "   print(\"=== Lịch sử commit của bảng (mới nhất trước) ===\")\n",
    "   \n",
    "   # Xem danh sách snapshots\n",
    "   snap_df = spark.table(\"hive_catalog.default.users.snapshots\")\n",
    "   snap_df.show(truncate=True)\n",
    "   print(\"=== Danh sách snapshot hiện tại ===\")\n",
    "   \n",
    "   spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113e9a97-7c8e-4ad0-90cc-357929d45f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark://spark-master:7077'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298df854-3e58-42a0-a688-844a65ec54d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/31 08:34:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/31 08:34:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m    spark.stop()\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m    \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Liệt kê các snapshot hiện có\u001b[39;00m\n\u001b[32m     18\u001b[39m snapshots_df = spark.sql(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT snapshot_id, committed_at, summary FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatalog_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.snapshots\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m snapshots_df.orderBy(\u001b[33m\"\u001b[39m\u001b[33mcommitted_at\u001b[39m\u001b[33m\"\u001b[39m).show(truncate=\u001b[43mT\u001b[49m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Danh sách snapshots ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Giả sử chúng ta lấy snapshot đầu tiên (cũ nhất) để time-travel:\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "   spark = (\n",
    "       SparkSession.builder\n",
    "           .appName(\"IcebergTimeTravelExample\") \\\n",
    "           .master(\"spark://spark-master:7077\") \\\n",
    "           .enableHiveSupport() \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "           .getOrCreate()\n",
    "   )\n",
    "   \n",
    "   catalog_table = \"hive_catalog.default.users\"\n",
    "   \n",
    "   # Liệt kê các snapshot hiện có\n",
    "   snapshots_df = spark.sql(f\"SELECT snapshot_id, committed_at, summary FROM {catalog_table}.snapshots\")\n",
    "   snapshots_df.orderBy(\"committed_at\").show(truncate=True)\n",
    "   print(\"=== Danh sách snapshots ===\")\n",
    "   \n",
    "   # Giả sử chúng ta lấy snapshot đầu tiên (cũ nhất) để time-travel:\n",
    "   first_snapshot_id = snapshots_df.orderBy(\"committed_at\").first()[\"snapshot_id\"]\n",
    "   print(f\">>> Sẽ time-travel về snapshot_id = {first_snapshot_id}\")\n",
    "   \n",
    "   # Đọc dữ liệu tại snapshot đó (version-as-of)\n",
    "   df_time_travel = spark.read \\\n",
    "       .format(\"iceberg\") \\\n",
    "       .option(\"snapshot-id\", first_snapshot_id) \\\n",
    "       .load(catalog_table)\n",
    "   df_time_travel.show(truncate=False)\n",
    "   print(f\"=== Dữ liệu tại snapshot {first_snapshot_id} ===\")\n",
    "   \n",
    "   # Hoặc dùng SQL cú pháp VERSION AS OF\n",
    "   df_sql = spark.sql(f\"SELECT * FROM {catalog_table} VERSION AS OF {first_snapshot_id}\")\n",
    "   df_sql.show(truncate=False)\n",
    "   print(f\"=== (SQL) Dữ liệu tại snapshot {first_snapshot_id} ===\")\n",
    "   \n",
    "   # Time-travel theo timestamp (ví dụ 5 phút trước)\n",
    "   import datetime, pytz\n",
    "   ts = (datetime.datetime.now(pytz.UTC) - datetime.timedelta(minutes=5)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "   print(f\">>> Sẽ time-travel theo timestamp = {ts}\")\n",
    "   df_ts = spark.read \\\n",
    "       .format(\"iceberg\") \\\n",
    "       .option(\"timestamp-as-of\", ts) \\\n",
    "       .load(catalog_table)\n",
    "   df_ts.show(truncate=False)\n",
    "   print(f\"=== Dữ liệu tại timestamp {ts} ===\")\n",
    "   \n",
    "   spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98667974-2ca0-4132-89c2-d8cd561a71b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
